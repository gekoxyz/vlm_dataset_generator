{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f60030b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   8%|▊         | 1/12 [00:11<02:03, 11.21s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 12/12 [01:52<00:00,  9.35s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from transformers import AutoProcessor, Gemma3ForConditionalGeneration, BitsAndBytesConfig\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "os.environ['CUDA_HOME'] = '/usr/local/cuda-12.4'\n",
    "os.environ['PATH'] = f\"{os.environ['CUDA_HOME']}/bin:{os.environ['PATH']}\"\n",
    "os.environ['LD_LIBRARY_PATH'] = f\"{os.environ['CUDA_HOME']}/lib64:{os.environ.get('LD_LIBRARY_PATH', '')}\"\n",
    "\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "model_id = \"google/gemma-3-27b-it\"\n",
    "\n",
    "print(\"Loading model...\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    ")\n",
    "\n",
    "model = Gemma3ForConditionalGeneration.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=torch.bfloat16, \n",
    "    low_cpu_mem_usage=True, \n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config\n",
    ").eval()\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1344bc3c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[BASIC DESCRIPTION NOT AVAILABLE, FOCUS ONLY ON THE IMAGES]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     17\u001b[0m basic_object_description_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt4point_test_no_vec.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 18\u001b[0m gpt4point_basic_descriptions \u001b[38;5;241m=\u001b[39m \u001b[43mload_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbasic_object_description_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m generated_content \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m, in \u001b[0;36mload_json\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_json\u001b[39m(file_path):\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file: \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson\u001b[49m\u001b[38;5;241m.\u001b[39mload(file)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "image_names = ['000.png', '010.png', '015.png', '020.png']\n",
    "data_root = \"media7link/gpt4point_test/\"\n",
    "\n",
    "object_ids = [\"0fa42f5b83084f0eb32533b760c8d146\", \"73b1f58d52e24b5ca601f54bf33d85c6\", \"1e488ff902e34e62affd7961c88293bb\", \"3729b2dd716f4c89b87a192290295808\"]\n",
    "\n",
    "def load_json(file_path):\n",
    "    with open(file_path, 'r') as file: return json.load(file)\n",
    "\n",
    "def get_basic_description_by_object_id(data, object_id):\n",
    "    for item in data:\n",
    "        if item[\"object_id\"] == object_id:\n",
    "            for conversation in item[\"conversations\"]:\n",
    "                if conversation[\"from\"] == \"gpt\":\n",
    "                    return conversation[\"value\"]\n",
    "    return \"[BASIC DESCRIPTION NOT AVAILABLE, FOCUS ONLY ON THE IMAGES]\"\n",
    "\n",
    "basic_object_description_path = \"gpt4point_test_no_vec.json\"\n",
    "gpt4point_basic_descriptions = load_json(basic_object_description_path)\n",
    "\n",
    "generated_content = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f901673",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    }
   ],
   "source": [
    "for object_id in object_ids:\n",
    "    images_paths = [os.path.join(data_root, object_id, img_name) for img_name in image_names]\n",
    "    images = [Image.open(p) for p in images_paths]\n",
    "\n",
    "    basic_description = get_basic_description_by_object_id(gpt4point_basic_descriptions, object_id)\n",
    "\n",
    "    prompt = f\"\"\"You are a meticulous and precise scene decomposition engine. Your task is to analyze the scene depicted in the provided images and output a structured breakdown of its contents. You must adhere strictly to what is visually observable. Do not infer, guess, or assume any details that are not explicitly visible.\n",
    "\n",
    "You may optionally refer to the supplementary description below to disambiguate or more precisely name what is clearly visible. However, **all descriptions must remain grounded in visual evidence only**.\n",
    "\n",
    "Supplementary description: \"{basic_description}\"\n",
    "\n",
    "## Disambiguation of Similar Objects\n",
    "To avoid ambiguity, you **must not** use generic numbered labels like \"Object 1\" or \"Object 2\". Instead, you must create a descriptive name for each instance that uniquely identifies it based on its most obvious spatial relationship to another, unambiguous object in the scene.\n",
    "If no such unique relational description is possible (e.g. two objects near to each other), you may describe them as a single group (e.g. \"a pair of shoes\", \"two stones\").\n",
    "\n",
    "## Primary Objects\n",
    "Primary Objects refer to the major components described in the scene that can be independently segmented or referenced. These are distinct from minor elements or attached parts unless those parts are significant because they are not usually like that in the object (e.g. a detached wheel from a car would be primary, but a car's attached wheel would not).\n",
    "\n",
    "## Output Format\n",
    "\n",
    "### Object Inventory  \n",
    "\n",
    "List every distinct, clearly visible, Primary Object in the foreground of the scene. Use precise terminology.\n",
    "- [Object 1 Name]  \n",
    "- [Object 2 Name]  \n",
    "- [Object 3 Name]  \n",
    "...\n",
    "\n",
    "### Detailed Descriptions  \n",
    "For each object listed above, describe the following attributes if they are visible:  \n",
    "- Shape  \n",
    "- Number of identical repeating elements (e.g., if a pattern repeats 5 times, or if the object contains 3 identical buttons)\n",
    "- Visible features such as markings, text, or logos  \n",
    "\n",
    "If an attribute is not visually verifiable, omit it rather than speculate. Refer to the supplementary description only to resolve naming ambiguities when the visual evidence supports it.\n",
    "\n",
    "- [Object 1 Name]: [Detailed visual attributes as above.]  \n",
    "- [Object 2 Name]: [Detailed visual attributes as above.]  \n",
    "...\n",
    "\n",
    "### Spatial Relationships  \n",
    "Describe the spatial layout and relative positions of the Primary Objects listed above using the reference frame of each Primary Object. This means describing other objects relative to how they would appear from the point of view of the Primary Object itself, and not from the image perspective.\n",
    "\n",
    "Use clear and consistent prepositions (e.g. \"to the left of,\" \"on top of,\" \"behind\") relative to the object's orientation. If an object has an identifiable front, sides, or top/bottom based on its form, use that intrinsic orientation to describe spatial relationships.\n",
    "\n",
    "Include both pairwise relationships and global placements within the scene. If an object appears in multiple positions (e.g. stacked, grouped), describe that clearly.\n",
    "\n",
    "Be unambiguous and specific. Prioritize spatial relationships between Primary Objects that are important for understanding the physical layout of the scene.\n",
    "\"\"\"\n",
    "\n",
    "    question = \"\"\"Please analyze the scene using the given instructions.\"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": prompt}]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                *[{\"type\": \"image\", \"image\": img} for img in images],\n",
    "                {\"type\": \"text\", \"text\": question}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # process with VLM\n",
    "    inputs = processor.apply_chat_template(\n",
    "        messages, add_generation_prompt=True, tokenize=True,\n",
    "        return_dict=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    inputs = inputs.to(model.device)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        generation = model.generate(**inputs, max_new_tokens=512, do_sample=False)\n",
    "\n",
    "    decoded = processor.decode(generation[0], skip_special_tokens=True)\n",
    "    model_response = decoded.split('\\nmodel\\n', 1)[1]\n",
    "\n",
    "    item_data = {\n",
    "        \"item_id\": object_id,\n",
    "        \"basic_description\": basic_description,\n",
    "        \"augmented_description\": model_response\n",
    "    }\n",
    "    generated_content.append(item_data)\n",
    "\n",
    "output_filename = \"smol_v3\"\n",
    "\n",
    "basic_description = \"BASIC_DESCRIPTION\"\n",
    "generated_content_wprompt = {\n",
    "    \"prompt\": [f\"{prompt}\\n{question}\"],\n",
    "    \"items\": generated_content\n",
    "}\n",
    "\n",
    "with open(f\"{output_filename}.json\", 'w') as f:\n",
    "    json.dump(generated_content_wprompt, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
